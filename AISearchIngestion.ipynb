{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24659730-6de8-4eb9-b2b1-570fca4b42c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install more-itertools azure-identity azure-search-documents==11.6.0b4 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37623ff8-f665-42b5-8bfe-1a661dc8a9fb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-13T05:39:35.0345171Z",
       "execution_start_time": "2025-05-13T05:39:12.3446992Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "56076d26-b6ff-4601-a77f-e21663503d48",
       "queued_time": "2025-05-13T05:35:19.7204371Z",
       "session_id": "36a557d1-dd5b-4df8-a6bd-5fc3a7896df6",
       "session_start_time": "2025-05-13T05:35:19.7215051Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 36a557d1-dd5b-4df8-a6bd-5fc3a7896df6, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "# Service Principal should have Cosmos DB Operation role on Cosmos DB account\n",
    "\n",
    "os.environ[\"AZURE_TENANT_ID\"] = \"<YOUR_TENANT_ID>\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = \"<YOUR_CLIENT_ID>\"\n",
    "sp_client_secret = notebookutils.credentials.getSecret('https://<YOUR_KEYVAULT>.vault.azure.net/', '<YOUR_SECRET_NAME>')\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = sp_client_secret\n",
    "\n",
    "\n",
    "credential=DefaultAzureCredential()\n",
    "token = credential.get_token(\"https://cosmos.azure.com/.default\").token\n",
    "\n",
    "index_name = \"books_index\"\n",
    "azure_search_endpoint = \"https://<YOUR_SEARCH_SERVICE_NAME>.search.windows.net\"\n",
    "search_client = SearchClient(endpoint=azure_search_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "search_token = credential.get_token(\"https://search.azure.com/.default\").token\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<AZURE_OPENAI_RESOURCE_NAME>.openai.azure.com/\"\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "AZURE_COSMOS_ENDPOINT = \"https://<YOUR_COSMOS_DB_ACCOUNT>.documents.azure.com:443/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "   api_version=\"2024-02-15-preview\",\n",
    "   azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "   azure_ad_token_provider=token_provider,   \n",
    ")\n",
    "\n",
    "# Set configuration settings\n",
    "config = {\n",
    "  \"spark.cosmos.accountEndpoint\": AZURE_COSMOS_ENDPOINT,\n",
    "  \"spark.cosmos.auth.type\": \"ServicePrincipal\",\n",
    "  \"spark.cosmos.account.subscriptionId\": \"<YOUR_SUBSCRIPTION_ID>\",\n",
    "  \"spark.cosmos.account.resourceGroupName\": \"<YOUR_RESOURCE_GROUP_NAME>\",\n",
    "  \"spark.cosmos.account.tenantId\": os.environ[\"AZURE_TENANT_ID\"],\n",
    "  \"spark.cosmos.auth.aad.clientId\": os.environ[\"AZURE_CLIENT_ID\"],\n",
    "  \"spark.cosmos.auth.aad.clientSecret\": sp_client_secret,\n",
    "  \"spark.cosmos.database\": \"YOUR_DATABASE_NAME\",\n",
    "  \"spark.cosmos.container\": \"YOUR_CONTAINER_NAME\",        \n",
    "}\n",
    "\n",
    "# Configure Catalog Api\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", config[\"spark.cosmos.accountEndpoint\"])\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.auth.type\", \"ServicePrincipal\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.account.subscriptionId\", config[\"spark.cosmos.account.subscriptionId\"])\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.account.resourceGroupName\", config[\"spark.cosmos.account.resourceGroupName\"])\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.account.tenantId\", config[\"spark.cosmos.account.tenantId\"])\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.auth.aad.clientId\", config[\"spark.cosmos.auth.aad.clientId\"])\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.auth.aad.clientSecret\", sp_client_secret)\n",
    "df = spark.read.format(\"cosmos.oltp\").options(**config).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ab4de9-a0c0-410e-bd8f-f05f6c23024c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-11T03:46:16.1023324Z",
       "execution_start_time": "2025-05-11T03:46:15.3277976Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2676b585-6eb1-434c-880f-a93ece23a849",
       "queued_time": "2025-05-11T03:46:08.7166295Z",
       "session_id": "eed4bfe5-69e5-4c1d-9055-f72fed064e39",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, eed4bfe5-69e5-4c1d-9055-f72fed064e39, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a database using the Catalog API\n",
    "#spark.sql(\"CREATE DATABASE IF NOT EXISTS cosmosCatalog.{};\".format(config[\"spark.cosmos.database\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fca44-b29b-4379-81f5-973986e3899d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Create container\n",
    "\n",
    "#spark.sql(\"CREATE TABLE IF NOT EXISTS cosmosCatalog.{}.{} USING cosmos.oltp TBLPROPERTIES(partitionKeyPath = '{}', manualThroughput = '{}')\".format(config[\"spark.cosmos.database\"], config[\"spark.cosmos.container\"], \"/session_id\", \"400\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b618b3-9e33-4988-ba61-a71772540e51",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-13T06:51:11.8392095Z",
       "execution_start_time": "2025-05-13T06:51:10.3856836Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "9bc27d94-8be5-43f1-abc4-0ba3704de761",
       "queued_time": "2025-05-13T06:51:10.3843555Z",
       "session_id": "36a557d1-dd5b-4df8-a6bd-5fc3a7896df6",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 23,
       "statement_ids": [
        23
       ]
      },
      "text/plain": [
       "StatementMeta(, 36a557d1-dd5b-4df8-a6bd-5fc3a7896df6, 23, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index created successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def create_simple_index(index_name: str, search_token: str, azure_search_endpoint: str):\n",
    "    index_schema = {\n",
    "        \"name\": index_name,\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"id\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"key\": True,\n",
    "                \"sortable\": True,\n",
    "                \"filterable\": True,\n",
    "                \"facetable\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"content\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"fileName\",\n",
    "                \"type\": \"Edm.String\",\n",
    "                \"searchable\": True\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"contentVector\",\n",
    "                \"type\": \"Collection(Edm.Single)\",\n",
    "                \"searchable\": True,\n",
    "                \"dimensions\": 1536,\n",
    "                \"vectorSearchProfile\": \"amlHnswProfile\"\n",
    "            }\n",
    "        ],\n",
    "        \"scoringProfiles\": [],\n",
    "        \"suggesters\": [],\n",
    "        \"vectorSearch\": {\n",
    "            \"algorithms\": [\n",
    "                {\n",
    "                    \"name\": \"amlHnsw\",\n",
    "                    \"kind\": \"hnsw\",\n",
    "                    \"hnswParameters\": {\n",
    "                        \"m\": 4,\n",
    "                        \"metric\": \"cosine\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"profiles\": [\n",
    "                {\n",
    "                    \"name\": \"amlHnswProfile\",\n",
    "                    \"algorithm\": \"amlHnsw\"\n",
    "                }\n",
    "            ],\n",
    "            \"vectorizers\": []\n",
    "        },\n",
    "        \"semantic\": {\n",
    "            \"configurations\": [\n",
    "                {\n",
    "                    \"name\": \"aml-semantic-config\",\n",
    "                    \"prioritizedFields\": {\n",
    "                        \"titleField\": {\"fieldName\": \"content\"},\n",
    "                        \"prioritizedKeywordsFields\": [{\"fieldName\": \"content\"}],\n",
    "                        \"prioritizedContentFields\": [{\"fieldName\": \"content\"}]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {search_token}\"\n",
    "    }\n",
    "\n",
    "    url = f\"{azure_search_endpoint}/indexes/{index_name}?api-version=2024-07-01\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 404:\n",
    "        create_response = requests.put(url, headers=headers, json=index_schema)\n",
    "        if create_response.status_code in [200, 201]:\n",
    "            print(\"✅ Index created successfully.\")\n",
    "        else:\n",
    "            print(\"❌ Failed to create index:\", create_response.text)\n",
    "    elif response.status_code == 200:\n",
    "        print(\"ℹ️ Index already exists.\")\n",
    "    else:\n",
    "        print(\"❌ Unexpected error while checking index:\", response.text)\n",
    "\n",
    "# Example usage:\n",
    "search_token = credential.get_token(\"https://search.azure.com/.default\").token\n",
    "index_name = \"YOUR_INDEX_NAME\"\n",
    "create_simple_index(index_name, search_token, azure_search_endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c496ee-5659-4431-9f7d-505080ffbf19",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-13T07:00:13.5423959Z",
       "execution_start_time": "2025-05-13T06:52:09.1908502Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b417b5ed-f4e9-4c6b-af1c-0698d30b951a",
       "queued_time": "2025-05-13T06:52:09.1894638Z",
       "session_id": "36a557d1-dd5b-4df8-a6bd-5fc3a7896df6",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 24,
       "statement_ids": [
        24
       ]
      },
      "text/plain": [
       "StatementMeta(, 36a557d1-dd5b-4df8-a6bd-5fc3a7896df6, 24, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import Row\n",
    "from more_itertools import chunked\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from random import uniform\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "\n",
    "\n",
    "# Constants\n",
    "#MAX_RETRIES = 50000\n",
    "INITIAL_BACKOFF = 2\n",
    "EMBEDDING_BATCH_SIZE = 16\n",
    "SEARCH_UPLOAD_BATCH_SIZE = 16\n",
    "NUM_THREADS = 4  # Number of threads per worker node\n",
    "\n",
    "sp_client_secret = notebookutils.credentials.getSecret(\n",
    "    'https://kv-anildwaa684447902659.vault.azure.net/',\n",
    "    'fabric-sp-client-secret'\n",
    ")\n",
    "sp_client_secret_bc = spark.sparkContext.broadcast(sp_client_secret)\n",
    "\n",
    "# Broadcast Azure OpenAI endpoint and index name\n",
    "\n",
    "\n",
    "#AZURE_OPENAI_ENDPOINT_bc = spark.sparkContext.broadcast(AZURE_OPENAI_ENDPOINT)\n",
    "#index_name_bc = spark.sparkContext.broadcast(index_name)\n",
    "\n",
    "# Load checkpoint from ADLS Gen2\n",
    "checkpoint_location = \"abfss://<YOUR_STORAGE_CONTAINER>@<YOUR_STORAGE_ACCOUNT>.dfs.core.windows.net/search_ingestion_checkpoint/\"\n",
    "checkpoint_file_path = checkpoint_location + \"completed_ids.json\"\n",
    "\n",
    "try:\n",
    "    checkpoint_df = spark.read.json(checkpoint_file_path)\n",
    "    completed_ids = set(row[\"id\"] for row in checkpoint_df.select(\"id\").collect())\n",
    "except Exception:\n",
    "    # Define empty DataFrame with the expected schema\n",
    "    checkpoint_schema = StructType([StructField(\"id\", StringType(), True)])\n",
    "    checkpoint_df = spark.createDataFrame([], checkpoint_schema)\n",
    "    completed_ids = set()\n",
    "\n",
    "# Filter documents to process\n",
    "#df_pending = df.filter(~df.id.isin(list(completed_ids)))\n",
    "df_pending = df.join(checkpoint_df, on=\"id\", how=\"left_anti\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clean text utility\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", text)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r\":\", \" -\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "import asyncio\n",
    "from openai import AsyncAzureOpenAI\n",
    "from azure.search.documents.aio import SearchClient as AsyncSearchClient\n",
    "from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import backoff\n",
    "from openai import RateLimitError\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.constant,\n",
    "    RateLimitError,\n",
    "    interval=60,\n",
    "    max_tries=MAX_RETRIES,\n",
    "    jitter=None\n",
    ")\n",
    "async def get_embeddings_with_retry(aoai_client, texts):\n",
    "    return await aoai_client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "# Process a chunk of rows\n",
    "async def process_row_chunk_async(row_chunk, aoai_client, search_client):\n",
    "    try:\n",
    "        texts = [row[\"text\"] for row in row_chunk]\n",
    "        #embeddings = await aoai_client.embeddings.create(\n",
    "        #    input=texts,\n",
    "        #    model=\"text-embedding-ada-002\"\n",
    "        #)\n",
    "        embeddings = await get_embeddings_with_retry(aoai_client, texts)\n",
    "        vectors = [e.embedding for e in embeddings.data]\n",
    "        documents_to_upload = [\n",
    "            {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"fileName\": row[\"fileName\"],\n",
    "                \"content\": clean_text(row[\"text\"]),\n",
    "                \"contentVector\": vector\n",
    "            }\n",
    "            for row, vector in zip(row_chunk, vectors)\n",
    "        ]\n",
    "        await search_client.upload_documents(documents=documents_to_upload)\n",
    "        return [row[\"id\"] for row in row_chunk], []\n",
    "    except Exception as e:\n",
    "        print(f\"Async batch failed: {e}\")\n",
    "        return [], [\n",
    "            Row(id=row[\"id\"], fileName=row[\"fileName\"], text=row[\"text\"], error=str(e))\n",
    "            for row in row_chunk\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "# Worker partition function\n",
    "def process_partition(rows):\n",
    "    import nest_asyncio\n",
    "\n",
    "    \n",
    "    nest_asyncio.apply()  # Required in notebooks\n",
    "    os.environ[\"AZURE_TENANT_ID\"] = \"<YOUR_TENANT_ID>\"\n",
    "    os.environ[\"AZURE_CLIENT_ID\"] = \"<YOUR_CLIENT_ID>\"\n",
    "    os.environ[\"AZURE_CLIENT_SECRET\"] = sp_client_secret_bc.value\n",
    "\n",
    "    async def run_partition():\n",
    "        credential = DefaultAzureCredential()\n",
    "        token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "        aoai_client = AsyncAzureOpenAI(\n",
    "            api_version=\"2024-02-15-preview\",\n",
    "            azure_endpoint=\"https://<YOUR_AZURE_OPENAN_RESOURCE_NAME>.openai.azure.com\",\n",
    "            azure_ad_token_provider=token_provider\n",
    "        )\n",
    "        search_client = AsyncSearchClient(\n",
    "            endpoint=\"https://<YOUR_SEARCH_SERVICE_NAME>.search.windows.net\",\n",
    "            index_name=\"<YOUR_INDEX_NAME>\",\n",
    "            credential=credential\n",
    "        )\n",
    "\n",
    "        checkpoint_updates = []\n",
    "        failed_rows = []\n",
    "\n",
    "        tasks = []\n",
    "        for chunk in chunked(list(rows), SEARCH_UPLOAD_BATCH_SIZE):\n",
    "            tasks.append(\n",
    "                process_row_chunk_async(chunk, aoai_client, search_client)\n",
    "            )\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for success_ids, failures in results:\n",
    "            checkpoint_updates.extend(success_ids)\n",
    "            failed_rows.extend(failures)\n",
    "\n",
    "        return [(\"success\", id) for id in checkpoint_updates] + [(\"failed\", r) for r in failed_rows]\n",
    "\n",
    "    return iter(asyncio.run(run_partition()))\n",
    "\n",
    "\n",
    "results = df_pending.rdd.mapPartitions(process_partition).collect()\n",
    "\n",
    "success_ids = [r[1] for r in results if r[0] == \"success\"]\n",
    "failed_docs = [r[1] for r in results if r[0] == \"failed\"]\n",
    "\n",
    "# Append successful IDs to checkpoint\n",
    "if success_ids:\n",
    "    df_checkpoint = spark.createDataFrame([Row(id=id) for id in success_ids])\n",
    "    df_checkpoint.write.mode(\"append\").json(checkpoint_file_path)\n",
    "\n",
    "failure_log_path = \"abfss://<YOUR_STORAGE_CONTAINER>@<YOUR_STORAGE_ACCOUNT>.dfs.core.windows.net/search_ingestion_failures/\"\n",
    "# Save failed docs\n",
    "if failed_docs:\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"fileName\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"error\", StringType(), True),\n",
    "    ])\n",
    "    df_failures = spark.createDataFrame(failed_docs, schema=schema)\n",
    "    df_failures.write.mode(\"append\").json(failure_log_path)\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "f26ed01d-0f6d-4a6e-8973-4d850b21d7b3",
    "workspaceId": "b1a1dad3-61f0-4438-be14-1651717fcaf7"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
